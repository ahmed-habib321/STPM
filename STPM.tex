\documentclass[]{article}


\input{MyTools}
\usepackage{fancyhdr}
\usepackage{fontspec}
\usepackage{physics}


%\definecolor{cover}{RGB}{230, 194, 24}
\definecolor{cover}{RGB}{51,215,213}
\newfontfamily\Acmefont{Acme-Regular}[
        Path = fonts/Acme/,
        Extension =.ttf
]
\newfontfamily\fancyfont{GreatVibes-Regular}[
        Path = fonts/Great_Vibes/,
        Extension =.ttf
]
\newfontfamily\handfont{PlaypenSans-ExtraBold}[
        Path = fonts/Playpen_Sans/,
        Extension =.ttf
]
\newfontfamily\Garamondfont{EBGaramond-Medium}[
        Path = fonts/Garamond/,
        Extension =.ttf
]

\begin{document}

\include{SoloLecStarter}   %When making solo lecture remove the comment
%\include{BookStarter}      %The pages in the front
%\include{BookBody}         %The content of the book

% \section{Generalized Inverse}

% In the classical sence of linear algebra, an $n$-by-$n$ square matrix $A$ is 
% called invertible also nonsingular 
% if there exists an $n$-by-$n$ square matrix B such that
% \[
% AB = BA = I_n        
% \]
% If this is the case, then the matrix $B$ is 
% uniquely determined by $A$, and is called the 
% inverse of $A$, denoted by $A^{-1}$
% \\
% In linear algebra for matrix $A$ to own an inverse 
% it had to be square matrix and nonsingular \\ (i.e $\det(A)\neq0$)
% \\
% Let us now study the case for any matrix of order $(m\times n)$ \\
% we define the generalized inverse $A^{+}$
% \begin{align*}
%         \circled{I}& \dquad AA^{+}A = A
%         \\
%         \circled{II}& \dquad \exists  U,V \quad \text{s.t} \quad A^{+} = UA^{*} \quad,\quad A^{+} = A^{*}V
% \end{align*}
% where $A^{*}$ is the conjugate transpose if all elements of $A$ are real then $A^{*} = A^{\operatorname{T}}$
% \[
% A = \begin{pmatrix}
%         1+2i & 3-i\\
%         5 & 6i \\
%     \end{pmatrix}
% \quad\Longrightarrow \quad
% A^{*} = \begin{pmatrix}
%         1-2i & 5\\
%         3+i & -6i \\
%     \end{pmatrix}
% \]
% \begin{theorem}[The existence and uniqueness of GI]
%         for matrix $A$ of order $(m\times n)$ the generalized inverse $A^{+}$ exists ,and it's unique
% \end{theorem}
% \begin{figure*}[b]
%         \begin{enrichment}{E.H. Moore}{moore.jpg}{2.4}{.8}{.17}
%         Eliakim Hastings Moore (1862–1932) was an American mathematician known for his contributions to algebra and mathematical logic. 
%         One of his significant contributions was in the field of linear algebra, 
%         particularly his work on the generalized inverse of a non-square matrix.
%         The Moore–Penrose inverse is a concept that extends the idea of the matrix inverse to non-square matrices.
%         The Moore–Penrose generalized inverse is perhaps the most well-known and widely used among 
%         various generalizations of the matrix inverse for non-square matrices. 
%         It has applications in solving linear systems of equations, 
%         least squares problems, and in situations where the original matrix might not have a unique inverse.
% \end{enrichment}
% \end{figure*}
% \begin{proof}[Proof the uniqueness]
%         suppose that there is two generalized inverse $A_1^{+}$ , $A_2^{+}$ thus
%         \begin{align*}
%                 \circled{I}& \dquad AA_i^{+}A = A
%                 \\
%                 \circled{II}& \dquad \exists  U_i,V_i \quad \text{s.t} \quad A_i^{+} = U_i A^{*} \quad,\quad A^{+} = A_i^{*}V_i \dquad \text{for }i=1,2
%         \end{align*}
% from \circled{I} 
% \begin{align*}
%         A[A_2^{+}-A_1^{+}]A =& 0
%         \\
%         AD^{+}A=&0
% \end{align*}
% where $D^{+} = A_2^{+}-A_1^{+}$ and we can say that 
% \begin{equation*}
%         \begin{aligned}[c]
%             &D^{+} = A^{*}V
%             \\
%             &V = V_2-V_1
%         \end{aligned}
%         \qquad\qquad
%         \begin{aligned}[c]
%         &D^{+} = UA^{*}
%         \\
%         &U = U_2-U_1
%         \end{aligned}
%     \end{equation*}
% now
% \begin{align*}
%         (D^{+}A)^{*}D^{+}A &= A^* D^{+^{\textstyle *}} D^{+} A
%         \\
%         &= A^* V^{*}A D^{+} A = 0
% \end{align*}
% because $AD^{+}A=0$, then we get 
% \begin{align*}
%         (D^{+}A)^{*}D^{+}A &= 0
%         \\
%         D^{+}A &= 0
%         \\
%         D^{+}A U^*&= 0
%         \\
%         D^{+}D^{+^{\textstyle *}}&= 0
%         \\
%         \therefore D^{+} &= 0
%         \\
%         A_2^{+}-A_1^{+} &= 0
%         \\
%         A_2^{+} &=A_1^{+}
% \end{align*}
% \end{proof}
% \begin{figure*}[b]
%         \begin{enrichment}{Roger Penrose}{Roger_Penrose.jpg}{2.4}{.8}{.17}
%         Sir Roger Penrose is a British mathematical physicist, mathematician, and philosopher 
%         who has made significant contributions to various fields, including general relativity, 
%         cosmology, and the foundations of quantum mechanics. \\
%         Penrose has made significant contributions to the fields of mathematics
%         One of Penrose's notable contributions is his work on the generalized inverse of a matrix, 
%         Penrose introduced the concept of the Moore-Penrose pseudoinverse, 
%         which is a widely used method for finding a generalized inverse of a matrix. 
%         This pseudoinverse has applications in various areas, including linear algebra, 
%         statistics, signal processing, and machine learning.
% \end{enrichment}
% \end{figure*}
% \begin{proof}[Proof the existence synthetically]
% Let $A$ be a non square matrix we can write $A = BC$ where
%         \begin{align*}
%                 A &\text{ is of order } (m\times n)
%                 \\
%                 B &\text{ is of order } (m\times r)
%                 \\
%                 C &\text{ is of order } (r\times n)
%         \end{align*}
% set
%         \begin{equation*}
%                 \begin{cases}
%                         \displaystyle \Lambda = C^{*}(CC^{*})^{-1}(B^{*}B)^{-1}B^{*}
%                         \\
%                         \displaystyle U = C^{*}(CC^{*})^{-1}(B^{*}B)^{-1}(CC^{*})^{-1}C
%                         \\
%                         \displaystyle V = B(B^{*}B)^{-1}(CC^{*})^{-1}(B^{*}B)^{-1}B^{*}
%                     \end{cases}
%         \end{equation*}
% we can see that 
%         \begin{align*}
%                 A\Lambda A &= BCC^{*}(CC^{*})^{-1}(B^{*}B)^{-1}B^{*}BC
%                 \\
%                 &= B\underbrace{\left[CC^{*}(CC^{*})^{-1}\right]}_{I}\underbrace{\left[(B^{*}B)^{-1}B^{*}B\right]}_{I}C
%                 \\
%                 &= BC = A
%         \end{align*}
% now to proof the property \circled{II}
% \begin{align*}
%         UA^* &=  C^{*}(CC^{*})^{-1}(B^{*}B)^{-1}(CC^{*})^{-1}C C^* B^*
%         \\
%         &=  C^{*}(CC^{*})^{-1}(B^{*}B)^{-1}\underbrace{\left[(CC^{*})^{-1}C C^*\right]}_{I} B^*
%         \\
%         &= C^{*}(CC^{*})^{-1}(B^{*}B)^{-1}B^* = \Lambda
%         \\
%         \\
%         A^*V &=  C^* B^*B(B^{*}B)^{-1}(CC^{*})^{-1}(B^{*}B)^{-1}B^{*}
%         \\
%         &=  C^{*}\underbrace{\left[B^*B(B^{*}B)^{-1}\right]}_{I}(CC^{*})^{-1}(B^{*}B)^{-1}B^{*}
%         \\
%         &= C^{*}(CC^{*})^{-1}(B^{*}B)^{-1}B^* = \Lambda
% \end{align*}
% thus $\Lambda = A^{+} $ is the generalized inverse
% \end{proof}
% \begin{enrichment*}{Synthetic Proofs}
%         Synthetic proofs in mathematics refer to a style of proof that relies on creative and constructive methods rather than analytical or deductive reasoning. In a synthetic proof, mathematicians often use geometric or intuitive arguments to establish the truth of a statement. This approach involves constructing figures, diagrams, or models that illustrate the relationships and properties relevant to the mathematical concept being proven.
% \end{enrichment*}
% \begin{proof}[Proof the existence analytically]
%         Let $A$ be a non square matrix we can write $A = BC$ where
%         \begin{align*}
%                 A &\text{ is of order } (m\times n)
%                 \\
%                 B &\text{ is of order } (m\times r)
%                 \\
%                 C &\text{ is of order } (r\times n)
%         \end{align*}
%         let us try to find two matrices such $B^{+} , Q$ that 
%         \[
%         BB^{+} B = B \quad,\quad B^{+} = QB^*
%         \]
%         we notice that 
%         \[
%         BQB^{*}B = B \quad,\quad B^{*}BQB^{*}B = B^{*}B
%         \]
%         since $\det(B^{*}B) \neq 0$ and $B^{*}B$ is square thus $Q$ is the ordinary inverse of $B^{*}B$
%         \[
%         Q = {(B^{*}B)}^{-1}
%         \]
%         thus 
%         \[
%         B^{+} = {(B^{*}B)}^{-1}B^*  
%         \]
%         similarly
%         \[
%         C^{+} = C^*{(C^{*}C)}^{-1}
%         \]
%         which makes that 
%         \[
%         A^{+}  = C^{+}B^{+}= C^*{(C^{*}C)}^{-1}{(B^{*}B)}^{-1}B^*
%         \]
%         the rest of the proof as in the synthetically part 
% \end{proof}
% \begin{enrichment*}{Analytic Proofs}
%         Analytic proofs, in the context of mathematics, refer to a method of proof that relies on logical reasoning and the application of established mathematical principles and definitions. The term "analytic" is derived from "analysis," emphasizing the breakdown of a mathematical statement or proposition into simpler, well-understood components.
% \end{enrichment*}
% \newpage
% \begin{lemma}
%         $B^{*}B$,$CC^{*}$ are square matrices and $\det(B^{*}B) ,\det(CC^{*}) \neq 0 $
% \end{lemma}
% \begin{proof}[Proof]
%         it's clear that $B^{*}B$,$CC^{*}$ are square matrices of order $r$ now to proof that 
%         $\det(B^{*}B) ,\det(CC^{*}) \neq 0 $\\
%         consider the homogeneous system

%         \[
%         B^{*}B X = 0  \dquad,\dquad X = \begin{pmatrix}
%         x_1\\
%         x_2\\
%         \vdots\\
%         x_r\\
%         \end{pmatrix}        
%         \]
% we have that 
%         \begin{align*}
%                 B^{*}B X &= 0
%                 \\
%                 X^{*}B^{*}B X &= 0
%                 \\
%                 (BX)^{*}BX &= 0
%         \end{align*}
% set $BX = Y$
%         \begin{align*}
%                 Y^{*}Y &= 0
%                 \\
%                 y_1^{2} + y_2^{2} + \dots + y_r^{2} &= 0
%                 \\
%                 y_1^{2} = y_2^{2} = \dots = y_r^{2} &= 0
%                 \\
%                 \Longrightarrow BX &= 0 
%         \end{align*}
% this means that only the trevial solution solves this equation
% \\
% but $\rank(B)=r=$number of unknowns therefore $x_1 = x_2 = \dots = x_r = 0$
% also $\rank(B^{*}B)=r$ then $\det(B^{*}B) \neq 0$
% \end{proof}

% \begin{example}
%         find $A^{+}$ if 
%         \[
%         A = \begin{pmatrix}
%                 1 & 1 & 4\\
%                 2 & 1 & 2\\
%             \end{pmatrix} 
%             = 
%             \underbrace{
%                 \begin{pmatrix}
%                         1 & 0\\
%                         0 & 1\\
%                     \end{pmatrix}
%             }_B
%             \underbrace{
%                 \begin{pmatrix}
%                         1 & 1 & 4\\
%                         2 & 1 & 2\\
%                     \end{pmatrix} 
%             }_C
%         \]
% we have taken $B = I$ to make the calculation easier but most of the time it doesn't work 
% \\
% only works if $\det(AA^{*}) \neq 0$ in case of $\det(AA^{*}) = 0$ you need to find two matrices $B,C$ s.t
% $BC = A$ and $\det(CC^{*}) , \det(B^{*}B) \neq 0$
%         \begin{align*}
%                 A^{+} &=  C^{*}(CC^{*})^{-1} (B^{*}B)^{-1}B^{*} =  A^{*}(AA^{*})^{-1}
%                 \\
%                 &=
%                 \begin{pmatrix}
%                         1 & 2\\
%                         1 & 1\\
%                         4 & 2
%                     \end{pmatrix} 
%                 \left[
%                         \begin{pmatrix}
%                                 1 & 1 & 4\\
%                                 2 & 1 & 2\\
%                         \end{pmatrix} 
%                         \begin{pmatrix}
%                                 1 & 2\\
%                                 1 & 1\\
%                                 4 & 2
%                         \end{pmatrix}   
%                 \right]^{-1}
%                 \\
%                 &=
%                 \begin{pmatrix}
%                         1 & 2\\
%                         1 & 1\\
%                         4 & 2
%                     \end{pmatrix} 
%                         \begin{pmatrix}
%                                 18 & 11\\
%                                 11 & 9\\
%                         \end{pmatrix}^{-1}
%                 \\
%                 &=
%                 \begin{pmatrix}
%                         1 & 2\\
%                         1 & 1\\
%                         4 & 2
%                 \end{pmatrix} 
%                 \begin{pmatrix}
%                         \displaystyle \frac{9}{41} & \displaystyle  \frac{-11}{41}\\\\
%                         \displaystyle \frac{-11}{41} & \displaystyle   \frac{18}{41}\\
%                 \end{pmatrix}   
%                 \\
%                 &=
%                 \begin{pmatrix}
%                         \displaystyle \frac{-13}{41} & \displaystyle \frac{25}{41}\\\\
%                         \displaystyle \frac{-2}{41} & \displaystyle \frac{7}{41}\\\\
%                         \displaystyle \frac{14}{41} & \displaystyle \frac{-8}{41}
%                 \end{pmatrix}
%                 \\
%                 AA^{+} &= \begin{pmatrix}
%                         1 & 1 & 4\\
%                         2 & 1 & 2\\
%                     \end{pmatrix} \begin{pmatrix}
%                         \displaystyle \frac{-13}{41} & \displaystyle \frac{25}{41}\\\\
%                         \displaystyle \frac{-2}{41} & \displaystyle \frac{7}{41}\\\\
%                         \displaystyle \frac{14}{41} & \displaystyle \frac{-8}{41}
%                 \end{pmatrix} = \begin{pmatrix}
%                         1 & 0 \\
%                         0 & 1 \\
%                     \end{pmatrix}
%         \end{align*}
% $
% \begin{pmatrix}
%         \displaystyle \frac{-13}{41} & \displaystyle \frac{25}{41}\\\\
%         \displaystyle \frac{-2}{41} & \displaystyle \frac{7}{41}\\\\
%         \displaystyle \frac{14}{41} & \displaystyle \frac{-8}{41}
% \end{pmatrix}
% $ is the right inverse of $A$


% notice that if you take $B = A$ and $C = I$ will not give an answer because $\det(A^{*}A) = 0$
% \end{example}





\section{Well-Posed Problems}
In mathematics a well-posed problem is one for 
which the following properties hold:

\begin{enumerate}
        \item The problem has a solution
        \item The solution is unique
        \item The solution's behavior changes continuously with the initial conditions
\end{enumerate}
Examples of well-posed problems include the Dirichlet problem for Laplace's equation
\\
This definition of a well-posed problem comes from the work of Jacques Hadamard on mathematical modeling of physical phenomena.

Problems that are not well-posed in the sense of Hadamard are termed ill-posed

For example, the inverse heat equation, deducing a previous distribution of temperature from final data, is not well-posed in that the solution is highly sensitive to changes in the final data.

\subsection{Hadamard Laplace Example}
In the theory of partial differential equations, an example 
constructed by J. Hadamard, which shows the instability of 
the solution of the Cauchy problem for the Laplace equation 
with respect to small changes in the initial data, is of great importance. 
Hadamard’s example served as the beginning of a 
systematic study of ill-posed problems in mathematical physics. 
On the other hand, the study of the Cauchy problem for 
the Laplace equation arises from problems of geophysics. 
At the same time, the question arises whether the Cauchy problem 
is correct for other elliptic equations. 

We will see how Hadamard’s example showed the the instability of 
the solution of the Cauchy problem for the Laplace equation. 

\begin{figure*}[b]
        \begin{enrichment}{Jacques Hadamard}{Hadamard.jpg}{2.4}{.8}{.17}
        Jacques Hadamard (1865–1963) was French mathematician who made significant contributions 
        to various branches of mathematics. One of his notable contributions lies in the field of 
        partial differential equations and the study of well-posed and ill-posed problems.

        Hadamard's work helped establish fundamental principles in the study of mathematical problems. 
        Well-posed problems are essential for ensuring the stability and reliability of mathematical models
         used in various scientific disciplines.
\end{enrichment}
\end{figure*}
Consider the following Cauchy problems for the Laplace equation.

\begin{minipage}{.5\textwidth}
        \begin{equation}
                \begin{cases}
                        \displaystyle \pdv[2]{u(x,t)}{t} + \pdv[2]{u(x,t)}{x} = 0
                        \\
                        u(x,0) = 0
                        \\
                        \displaystyle \pdv{u(x,0)}{t} = \phi(x)
                \end{cases}        
        \end{equation}
\end{minipage}
\begin{minipage}{.5\textwidth}
        \begin{equation}
                \begin{cases}
                        \displaystyle \pdv[2]{u^{*}(x,t)}{t} + \pdv[2]{u^{*}(x,t)}{x} = 0
                        \\
                        u^{*}(x,0) = 0
                        \\
                        \displaystyle \pdv{u^{*}(x,0)}{t} = \phi(x) + \frac{\sin(nx)}{n^{k}}
                \end{cases}               
        \end{equation}
\end{minipage}


As we can see the two problems are identical except for the second initial condition in (2) but if we take the 
limit for $n \to \infty$ in the (2) it suppose to reduce to problem (1) which means that $u(x,t) = u^{*}(x,t)$ as $n \to \infty$

BUT

let's construct new problem using (1),(2)

let $V(x,t) = u^{*}(x,t) - u(x,t)$ we get that 
\begin{equation}
        \begin{cases}
                \displaystyle \pdv[2]{V(x,t)}{t} + \pdv[2]{V(x,t)}{x} = 0
                \\
                V(x,0) = 0
                \\
                \displaystyle \pdv{V(x,0)}{t} =\frac{\sin(nx)}{n^{k}}
        \end{cases}               
\end{equation}
the solution of this problem is given by 
\[
V(x,t) = \frac{\sinh(nt)\sin(nx)}{n^{k+1}}
\]
now as we Assumed $V(x,t)$ should vanish as $n \to \infty$ but we find that 
\[
\lim_{n \to \infty} V(x,t) = \infty
\]
that's because $\mathbf{O}(\sinh(n)) > \mathbf{O}(n^{k+1})$ which causes the numerator to diverges faster than the denominator
\setcounter{equation}{0}
\section{Some Basic Differential Models in Mathematical Biology}
\subsection{Population dynamic models (Malthus model)}
Any specie in the natural world does not exist solely, 
but is closely related to other species in biological 
communities and then constitute a population ecosystem. 
However,        a single population is the basic unit which 
composed of the entire ecosystem. In order to predict 
the change law of the population, the famous demographer 
Malthus proposed the following basic model for single 
specie
\begin{equation}
        \dv{N(t)}{t} =rN(t)
\end{equation}
Where $N(t)$ denotes the population density at the time $t$, $r$ denotes the intrinsic growth rate, which is the difference between the birth rate and the death rate.

Based on model (1), the famous ecologist Logistic proposed the famous insect population model when studying the growth rate of the insect in the laboratory as follows,
\[
        \dv{N(t)}{t} =rN(t)[1-\frac{N(t)}{K}]
\]
Where $r$ denotes the intrinsic growth rate, $K$ is the carrying capacity of the environment


\subsection{Epidemic models(SIR model)}
As the epidemic models are concerned, it should date back to the famous SIR model, 
which was proposed by Kermack and Mckendrick in 1927 when studying the propagation 
law of the Black Death in London from 1655-1666 and the plague in Mumbai in 1906. 
And the famous SIR model is as follow,
\begin{equation}
        \begin{cases}
                \displaystyle \dv{S(t)}{t} = -\beta S(t)I(t)
                \\\\
                \displaystyle \dv{I(t)}{t} = \beta S(t)I(t)-\gamma I(t)
                \\\\
                \displaystyle \dv{R(t)}{t} = \gamma I(t)
        \end{cases}               
\end{equation}
Where $S(t)$ denotes the number of the susceptible individuals which has not yet infected 
but may be infected by the bacteria at time $t$, $I(t)$ denotes the number of the infected 
individuals at time $t$, and $R(t)$ denotes the number of the removed individuals at time $t$, $\beta$, 
denotes the infected rate, $\gamma$ denotes the cure rate and $\gamma^{-1}$ denotes the average cure rate.

notice that if we add equations (2) we get 
\begin{align*}
        \dv{S(t)}{t} + \dv{I(t)}{t} + \dv{R(t)}{t} &= \beta S(t)I(t)-\gamma I(t) -\beta S(t)I(t) \gamma I(t) 
        \\
        \dv{S(t) + I(t) + R(t)}{t} &= 0
        \\
        S(t) + I(t) + R(t) &= constant
\end{align*}
that says that the total number of the individuals keeps constant. 
\\
and it's Assumed that the infected individuals have permanent immune capacity after the cure.

%\include{BookEnd}          %The pages in the End
\end{document}